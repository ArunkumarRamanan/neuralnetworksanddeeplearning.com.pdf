\babel@toc {english}{}
\contentsline {chapter}{What this book is about}{iii}{chapter*.2}
\contentsline {chapter}{On the exercises and problems}{v}{chapter*.3}
\contentsline {chapter}{\numberline {1}Using neural nets to recognize handwritten digits}{1}{chapter.1}
\contentsline {section}{\numberline {1.1}Perceptrons}{2}{section.1.1}
\contentsline {section}{\numberline {1.2}Sigmoid neurons}{7}{section.1.2}
\contentsline {section}{\numberline {1.3}The architecture of neural networks}{10}{section.1.3}
\contentsline {section}{\numberline {1.4}A simple network to classify handwritten digits}{12}{section.1.4}
\contentsline {section}{\numberline {1.5}Learning with gradient descent}{15}{section.1.5}
\contentsline {section}{\numberline {1.6}Implementing our network to classify digits}{24}{section.1.6}
\contentsline {section}{\numberline {1.7}Toward deep learning}{35}{section.1.7}
\contentsline {chapter}{\numberline {2}How the backpropagation algorithm works}{39}{chapter.2}
\contentsline {section}{\numberline {2.1}Warm up: a fast matrix-based approach to computing the output from a neural network}{40}{section.2.1}
\contentsline {section}{\numberline {2.2}The two assumptions we need about the cost function}{42}{section.2.2}
\contentsline {section}{\numberline {2.3}The Hadamard product, $s\odot {}t$}{43}{section.2.3}
\contentsline {section}{\numberline {2.4}The four fundamental equations behind backpropagation}{43}{section.2.4}
\contentsline {section}{\numberline {2.5}Proof of the four fundamental equations (optional)}{48}{section.2.5}
\contentsline {section}{\numberline {2.6}The backpropagation algorithm}{49}{section.2.6}
\contentsline {section}{\numberline {2.7}The code for backpropagation}{50}{section.2.7}
\contentsline {section}{\numberline {2.8}In what sense is backpropagation a fast algorithm?}{52}{section.2.8}
\contentsline {section}{\numberline {2.9}Backpropagation: the big picture}{53}{section.2.9}
\contentsline {chapter}{\numberline {3}Improving the way neural networks learn}{59}{chapter.3}
\contentsline {section}{\numberline {3.1}The cross-entropy cost function}{60}{section.3.1}
\contentsline {subsection}{\numberline {3.1.1}Introducing the cross-entropy cost function}{62}{subsection.3.1.1}
\contentsline {subsection}{\numberline {3.1.2}Using the cross-entropy to classify MNIST digits}{67}{subsection.3.1.2}
\contentsline {subsection}{\numberline {3.1.3}What does the cross-entropy mean? Where does it come from?}{68}{subsection.3.1.3}
\contentsline {subsection}{\numberline {3.1.4}Softmax}{70}{subsection.3.1.4}
\contentsline {section}{\numberline {3.2}Overfitting and regularization}{73}{section.3.2}
\contentsline {subsection}{\numberline {3.2.1}Regularization}{78}{subsection.3.2.1}
\contentsline {subsection}{\numberline {3.2.2}Why does regularization help reduce overfitting?}{83}{subsection.3.2.2}
\contentsline {subsection}{\numberline {3.2.3}Other techniques for regularization}{87}{subsection.3.2.3}
\contentsline {section}{\numberline {3.3}Weight initialization}{94}{section.3.3}
\contentsline {section}{\numberline {3.4}Handwriting recognition revisited: the code}{98}{section.3.4}
\contentsline {section}{\numberline {3.5}How to choose a neural network's hyper-parameters?}{107}{section.3.5}
\contentsline {section}{\numberline {3.6}Other techniques}{118}{section.3.6}
\contentsline {subsection}{\numberline {3.6.1}Variations on stochastic gradient descent}{118}{subsection.3.6.1}
\contentsline {chapter}{\numberline {4}A visual proof that neural nets can compute any function}{127}{chapter.4}
\contentsline {section}{\numberline {4.1}Two caveats}{129}{section.4.1}
\contentsline {section}{\numberline {4.2}Universality with one input and one output}{130}{section.4.2}
\contentsline {section}{\numberline {4.3}Many input variables}{139}{section.4.3}
\contentsline {section}{\numberline {4.4}Extension beyond sigmoid neurons}{146}{section.4.4}
\contentsline {section}{\numberline {4.5}Fixing up the step functions}{148}{section.4.5}
\contentsline {chapter}{\numberline {5}Why are deep neural networks hard to train?}{151}{chapter.5}
\contentsline {section}{\numberline {5.1}The vanishing gradient problem}{154}{section.5.1}
\contentsline {section}{\numberline {5.2}What's causing the vanishing gradient problem? Unstable gradients in deep neural nets}{159}{section.5.2}
\contentsline {section}{\numberline {5.3}Unstable gradients in more complex networks}{163}{section.5.3}
\contentsline {section}{\numberline {5.4}Other obstacles to deep learning}{164}{section.5.4}
\contentsline {chapter}{\numberline {6}Deep learning}{167}{chapter.6}
\contentsline {section}{\numberline {6.1}Introducing convolutional networks}{169}{section.6.1}
\contentsline {section}{\numberline {6.2}Convolutional neural networks in practice}{176}{section.6.2}
\contentsline {section}{\numberline {6.3}The code for our convolutional networks}{185}{section.6.3}
\contentsline {section}{\numberline {6.4}Recent progress in image recognition}{196}{section.6.4}
\contentsline {section}{\numberline {6.5}Other approaches to deep neural nets}{202}{section.6.5}
\contentsline {section}{\numberline {6.6}On the future of neural networks}{205}{section.6.6}
\contentsline {chapter}{\numberline {A}Is there a simple algorithm for intelligence?}{211}{appendix.A}
